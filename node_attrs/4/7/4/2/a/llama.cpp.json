{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "build_platform": {
   "osx_arm64": "osx_64"
  }
 },
 "feedstock_name": "llama.cpp",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "home": "https://github.com/ggerganov/llama.cpp",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Port of Facebook's LLaMA model in C/C++"
  },
  "build": {
   "number": "5",
   "script": [
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX"
   ],
   "string": "cuda120_h1234567_5"
  },
  "extra": {
   "recipe-maintainers": [
    "sodre",
    "sodre",
    "sodre"
   ]
  },
  "package": {
   "name": "llama.cpp",
   "version": "0.0.1601"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja"
   ],
   "host": [
    "cuda-cudart-dev",
    "libcublas-dev"
   ],
   "run": [
    "cuda-version 11.2",
    "cuda-version 11.8",
    "cuda-version 12.0",
    "cuda-cudart  12.0"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "829f7c3696fb9da15faf0b70e7d33a174d3de5111ac797511ca0974eb42cf572",
   "url": "https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1601.tar.gz"
  },
  "test": {
   "commands": [
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cmake",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "git",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "libcublas-dev"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-cudart",
    "cuda-version"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "meta_yaml": {
  "about": {
   "home": "https://github.com/ggerganov/llama.cpp",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Port of Facebook's LLaMA model in C/C++"
  },
  "build": {
   "number": "5",
   "script": [
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_ACCELERATE=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_METAL=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_AVX=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_AVX2=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_FMA=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_F16C=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_ACCELERATE=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_METAL=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "set LLAMA_ARGS",
    "cmake -S . -B build -G Ninja %LLAMA_ARGS%",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "set LLAMA_ARGS",
    "cmake -S . -B build -G Ninja %LLAMA_ARGS%",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "set LLAMA_ARGS",
    "cmake -S . -B build -G Ninja %LLAMA_ARGS%",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX"
   ],
   "string": "cuda118_h1234567_5"
  },
  "extra": {
   "recipe-maintainers": [
    "sodre",
    "sodre",
    "sodre",
    "sodre",
    "sodre",
    "sodre",
    "sodre",
    "sodre"
   ]
  },
  "package": {
   "name": "llama.cpp",
   "version": "0.0.1601"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja"
   ],
   "host": [
    "cuda-cudart-dev",
    "libcublas-dev",
    "cuda-cudart-dev",
    "libcublas-dev"
   ],
   "run": [
    "cuda-version 11.2",
    "cuda-version 11.8",
    "cuda-version 12.0",
    "cuda-cudart  12.0",
    "cuda-version 12.0",
    "cuda-cudart  12.0",
    "cuda-cudart_win-64 12.0",
    "cuda-version 11.2",
    "cuda-version 11.8"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "829f7c3696fb9da15faf0b70e7d33a174d3de5111ac797511ca0974eb42cf572",
   "url": "https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1601.tar.gz"
  },
  "test": {
   "commands": [
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help"
   ]
  }
 },
 "name": "llama.cpp",
 "osx_64_meta_yaml": {
  "about": {
   "home": "https://github.com/ggerganov/llama.cpp",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Port of Facebook's LLaMA model in C/C++"
  },
  "build": {
   "number": "5",
   "script": [
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_ACCELERATE=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_METAL=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX"
   ],
   "string": "mps_h1234567_5"
  },
  "extra": {
   "recipe-maintainers": [
    "sodre"
   ]
  },
  "package": {
   "name": "llama.cpp",
   "version": "0.0.1601"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cmake",
    "git",
    "ninja"
   ],
   "host": [],
   "run": []
  },
  "source": {
   "patches": [
    "osx-64-pick-discrete.patch"
   ],
   "sha256": "829f7c3696fb9da15faf0b70e7d33a174d3de5111ac797511ca0974eb42cf572",
   "url": "https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1601.tar.gz"
  },
  "test": {
   "commands": [
    "main --help",
    "server --help"
   ]
  }
 },
 "osx_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cmake",
    "cxx_compiler_stub",
    "git",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": []
  },
  "run": {
   "__set__": true,
   "elements": []
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "osx_arm64_meta_yaml": {
  "about": {
   "home": "https://github.com/ggerganov/llama.cpp",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Port of Facebook's LLaMA model in C/C++"
  },
  "build": {
   "number": "5",
   "script": [
    "LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_NATIVE=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_AVX=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_AVX2=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_AVX2=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_FMA=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_FMA=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_F16C=OFF\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_F16C=OFF",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_ACCELERATE=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_ACCELERATE=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_METAL=ON\"",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_METAL=ON",
    "LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_CUBLAS=ON\"",
    "echo $LLAMA_ARGS",
    "cmake -S . -B build -G Ninja ${LLAMA_ARGS}",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX"
   ],
   "string": "mps_h1234567_5"
  },
  "extra": {
   "recipe-maintainers": [
    "sodre"
   ]
  },
  "package": {
   "name": "llama.cpp",
   "version": "0.0.1601"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cmake",
    "git",
    "ninja"
   ],
   "host": [],
   "run": []
  },
  "source": {
   "patches": null,
   "sha256": "829f7c3696fb9da15faf0b70e7d33a174d3de5111ac797511ca0974eb42cf572",
   "url": "https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1601.tar.gz"
  },
  "test": {
   "commands": [
    "main --help",
    "server --help"
   ]
  }
 },
 "osx_arm64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cmake",
    "cxx_compiler_stub",
    "git",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": []
  },
  "run": {
   "__set__": true,
   "elements": []
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "outputs_names": {
  "__set__": true,
  "elements": [
   "llama.cpp"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64",
  "osx_64",
  "osx_arm64",
  "win_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/llama.cpp.json"
 },
 "raw_meta_yaml": "{% set name = \"llama.cpp\" %}\n{% set version = \"1601\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: 0.0.{{ version }}\n\nsource:\n  url: https://github.com/ggerganov/llama.cpp/archive/refs/tags/b{{ version }}.tar.gz\n  sha256: 829f7c3696fb9da15faf0b70e7d33a174d3de5111ac797511ca0974eb42cf572\n  patches:\n    - osx-64-pick-discrete.patch  # [osx and x86_64]\n\nbuild:\n  skip: True  # [(linux or win) and cuda_compiler_version == \"None\"]\n  number: 5\n  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != \"None\"]\n  string: cpu_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [cuda_compiler_version == \"None\"]\n  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [osx]\n\n  script:\n    - LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"              # [unix]\n    - set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF           # [win]\n    {% macro llama_args(value) -%}\n    - LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_{{ value }}\"     # [unix]\n    - set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_{{ value }}  # [win]\n    {%- endmacro %}\n\n    {{ llama_args(\"NATIVE=OFF\") }}      # [(osx and arm64) or win]\n    {{ llama_args(\"AVX=OFF\") }}         # [osx and arm64]\n    {{ llama_args(\"AVX2=OFF\") }}        # [osx and arm64]\n    {{ llama_args(\"FMA=OFF\") }}         # [osx and arm64]\n    {{ llama_args(\"F16C=OFF\") }}        # [osx and arm64]\n    {{ llama_args(\"ACCELERATE=ON\") }}   # [osx]\n    {{ llama_args(\"METAL=ON\") }}        # [osx]\n    {{ llama_args(\"CUBLAS=ON\") }}       # [cuda_compiler_version != \"None\"]\n\n    - echo $LLAMA_ARGS  # [unix]\n    - set LLAMA_ARGS    # [win]\n\n    - cmake -S . -B build -G Ninja %LLAMA_ARGS%   # [win]\n    - cmake -S . -B build -G Ninja ${LLAMA_ARGS}  # [unix]\n    - cmake --build build --config Release\n    - cmake --install build --prefix={{ PREFIX }}\n\nrequirements:\n  build:\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n    - {{ compiler('cuda') }}  # [cuda_compiler_version != \"None\"]\n    - cmake\n    - git\n    - ninja\n  host:\n    - cuda-cudart-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcublas-dev    # [(cuda_compiler_version or \"\").startswith(\"12\")]\n  run:\n    - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != \"None\"]\n    - cuda-cudart  {{ cuda_compiler_version }}  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - cuda-cudart_{{ target_platform }} {{ cuda_compiler_version }}  # [(cuda_compiler_version or \"\").startswith(\"12\") and win]\n\ntest:\n  commands:\n    - main --help\n    - server --help\n\nabout:\n  home: https://github.com/ggerganov/llama.cpp\n  summary: Port of Facebook's LLaMA model in C/C++\n  license: MIT\n  license_family: MIT\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - sodre\n",
 "req": {
  "__set__": true,
  "elements": [
   "c_compiler_stub",
   "cmake",
   "cuda-cudart",
   "cuda-cudart-dev",
   "cuda-cudart_win-64",
   "cuda-version",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "git",
   "libcublas-dev",
   "ninja"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cmake",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "git",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda-cudart-dev",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "libcublas-dev"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda-cudart",
    "cuda-cudart_win-64",
    "cuda-version",
    "cuda_compiler_stub",
    "cxx_compiler_stub"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cmake",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "git",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "libcublas-dev"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-cudart  12.0",
    "cuda-cudart_win-64 12.0",
    "cuda-version 11.2",
    "cuda-version 11.8",
    "cuda-version 12.0"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "url": "https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1601.tar.gz",
 "version": "0.0.1601",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/llama.cpp.json"
 },
 "win_64_meta_yaml": {
  "about": {
   "home": "https://github.com/ggerganov/llama.cpp",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Port of Facebook's LLaMA model in C/C++"
  },
  "build": {
   "number": "5",
   "script": [
    "set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "set LLAMA_ARGS",
    "cmake -S . -B build -G Ninja %LLAMA_ARGS%",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "set LLAMA_ARGS",
    "cmake -S . -B build -G Ninja %LLAMA_ARGS%",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX",
    "set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_NATIVE=OFF",
    "set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_CUBLAS=ON",
    "set LLAMA_ARGS",
    "cmake -S . -B build -G Ninja %LLAMA_ARGS%",
    "cmake --build build --config Release",
    "cmake --install build --prefix=PREFIX"
   ],
   "string": "cuda118_h1234567_5"
  },
  "extra": {
   "recipe-maintainers": [
    "sodre",
    "sodre",
    "sodre"
   ]
  },
  "package": {
   "name": "llama.cpp",
   "version": "0.0.1601"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "cmake",
    "git",
    "ninja"
   ],
   "host": [
    "cuda-cudart-dev",
    "libcublas-dev"
   ],
   "run": [
    "cuda-version 12.0",
    "cuda-cudart  12.0",
    "cuda-cudart_win-64 12.0",
    "cuda-version 11.2",
    "cuda-version 11.8"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "829f7c3696fb9da15faf0b70e7d33a174d3de5111ac797511ca0974eb42cf572",
   "url": "https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1601.tar.gz"
  },
  "test": {
   "commands": [
    "main --help",
    "server --help",
    "main --help",
    "server --help",
    "main --help",
    "server --help"
   ]
  }
 },
 "win_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cmake",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "git",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "libcublas-dev"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-cudart",
    "cuda-cudart_win-64",
    "cuda-version"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 }
}